"""
Evaluation and Visualization Script for Woodpecker Results

This script evaluates the results generated by the Woodpecker system and creates
comprehensive visualizations including:
- CLIP similarity distributions
- HCS score distributions
- Correction effectiveness analysis
- Comparison charts
"""

import json
import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple
import warnings
from scipy import stats as scipy_stats
warnings.filterwarnings('ignore')

# Set style for publication-quality plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['xtick.labelsize'] = 11
plt.rcParams['ytick.labelsize'] = 11
plt.rcParams['legend.fontsize'] = 11
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['savefig.bbox'] = 'tight'
plt.rcParams['font.family'] = 'serif'


def load_results(jsonl_path: str) -> pd.DataFrame:
    """Load results from JSONL file into pandas DataFrame."""
    print(f"üìñ Loading results from {jsonl_path}...")
    results = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line.strip()))
    
    df = pd.DataFrame(results)
    print(f"‚úÖ Loaded {len(df)} samples")
    return df


def compute_statistics(df: pd.DataFrame) -> Dict:
    """Compute comprehensive statistics from results."""
    stats = {}
    
    # Basic counts
    stats['total_samples'] = len(df)
    stats['unique_images'] = df['image'].nunique()
    
    # CLIP Similarity Statistics
    stats['clip_generated'] = {
        'mean': df['clip_sim_generated'].mean(),
        'std': df['clip_sim_generated'].std(),
        'min': df['clip_sim_generated'].min(),
        'max': df['clip_sim_generated'].max(),
        'median': df['clip_sim_generated'].median()
    }
    
    stats['clip_corrected'] = {
        'mean': df['clip_sim_corrected'].mean(),
        'std': df['clip_sim_corrected'].std(),
        'min': df['clip_sim_corrected'].min(),
        'max': df['clip_sim_corrected'].max(),
        'median': df['clip_sim_corrected'].median()
    }
    
    stats['clip_ground'] = {
        'mean': df['clip_sim_ground'].mean(),
        'std': df['clip_sim_ground'].std(),
        'min': df['clip_sim_ground'].min(),
        'max': df['clip_sim_ground'].max(),
        'median': df['clip_sim_ground'].median()
    }
    
    # Improvement metrics
    df['clip_improvement'] = df['clip_sim_corrected'] - df['clip_sim_generated']
    
    # Statistical tests for paper
    # Paired t-test: Generated vs Corrected
    t_stat, p_value = scipy_stats.ttest_rel(df['clip_sim_corrected'], df['clip_sim_generated'])
    
    # Effect size (Cohen's d)
    pooled_std = np.sqrt((df['clip_sim_corrected'].std()**2 + df['clip_sim_generated'].std()**2) / 2)
    cohens_d = (df['clip_sim_corrected'].mean() - df['clip_sim_generated'].mean()) / pooled_std
    
    stats['clip_improvement'] = {
        'mean': df['clip_improvement'].mean(),
        'std': df['clip_improvement'].std(),
        'improved_count': (df['clip_improvement'] > 0).sum(),
        'improved_percentage': (df['clip_improvement'] > 0).sum() / len(df) * 100,
        'degraded_count': (df['clip_improvement'] < 0).sum(),
        'degraded_percentage': (df['clip_improvement'] < 0).sum() / len(df) * 100,
        'unchanged_count': (df['clip_improvement'] == 0).sum(),
        't_statistic': float(t_stat),
        'p_value': float(p_value),
        'cohens_d': float(cohens_d),
        'significant': p_value < 0.05
    }
    
    # HCS Statistics
    if 'hcs_score' in df.columns:
        hcs_scores = df['hcs_score'].dropna()
        if len(hcs_scores) > 0:
            stats['hcs'] = {
                'mean': float(hcs_scores.mean()),
                'std': float(hcs_scores.std()),
                'min': float(hcs_scores.min()),
                'max': float(hcs_scores.max()),
                'median': float(hcs_scores.median()),
                'valid_count': len(hcs_scores),
                'missing_count': int(df['hcs_score'].isna().sum())
            }
            
            # HCS categories
            stats['hcs_categories'] = {
                'high_confidence': int((hcs_scores >= 0.7).sum()),
                'medium_confidence': int(((hcs_scores >= 0.4) & (hcs_scores < 0.7)).sum()),
                'low_confidence': int((hcs_scores < 0.4).sum())
            }
        else:
            stats['hcs'] = {
                'valid_count': 0,
                'missing_count': len(df)
            }
            stats['hcs_categories'] = {
                'high_confidence': 0,
                'medium_confidence': 0,
                'low_confidence': 0
            }
    
    # Text length statistics
    df['generated_length'] = df['generated_text'].str.len()
    df['corrected_length'] = df['corrected_output'].str.len()
    df['ground_truth_length'] = df['ground_truth'].str.len()
    
    stats['text_lengths'] = {
        'generated': {
            'mean': df['generated_length'].mean(),
            'median': df['generated_length'].median()
        },
        'corrected': {
            'mean': df['corrected_length'].mean(),
            'median': df['corrected_length'].median()
        },
        'ground_truth': {
            'mean': df['ground_truth_length'].mean(),
            'median': df['ground_truth_length'].median()
        }
    }
    
    # Correction changes
    df['text_changed'] = df['generated_text'] != df['corrected_output']
    stats['correction_stats'] = {
        'changed_count': df['text_changed'].sum(),
        'changed_percentage': df['text_changed'].sum() / len(df) * 100,
        'unchanged_count': (~df['text_changed']).sum()
    }
    
    return stats, df


def create_visualizations(df: pd.DataFrame, stats: Dict, output_dir: str):
    """Create comprehensive visualizations."""
    print(f"üìä Generating visualizations...")
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. CLIP Similarity Distribution Comparison
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Histogram
    axes[0].hist(df['clip_sim_generated'], bins=30, alpha=0.6, label='Generated', color='blue')
    axes[0].hist(df['clip_sim_corrected'], bins=30, alpha=0.6, label='Corrected', color='green')
    axes[0].hist(df['clip_sim_ground'], bins=30, alpha=0.6, label='Ground Truth', color='orange')
    axes[0].set_xlabel('CLIP Similarity Score')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('CLIP Similarity Distribution Comparison')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Box plot
    clip_data = [df['clip_sim_generated'], df['clip_sim_corrected'], df['clip_sim_ground']]
    axes[1].boxplot(clip_data, labels=['Generated', 'Corrected', 'Ground Truth'])
    axes[1].set_ylabel('CLIP Similarity Score')
    axes[1].set_title('CLIP Similarity Box Plot Comparison')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '1_clip_similarity_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: 1_clip_similarity_comparison.png")
    
    # 2. CLIP Improvement Analysis
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Improvement distribution
    axes[0].hist(df['clip_improvement'], bins=50, alpha=0.7, color='purple', edgecolor='black')
    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No Change')
    mean_improvement = df['clip_improvement'].mean()
    axes[0].axvline(x=mean_improvement, color='green', linestyle='--', linewidth=2, 
                    label=f'Mean: {mean_improvement:.4f}')
    
    # Add statistical significance annotation
    p_val = stats['clip_improvement']['p_value']
    sig_text = f"p = {p_val:.4f}" + ("***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns")
    axes[0].text(0.02, 0.98, sig_text, transform=axes[0].transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                fontsize=11, fontweight='bold')
    
    axes[0].set_xlabel('CLIP Improvement (Corrected - Generated)', fontsize=12)
    axes[0].set_ylabel('Frequency', fontsize=12)
    axes[0].set_title('CLIP Similarity Improvement Distribution', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)
    
    # Improvement categories
    improved = (df['clip_improvement'] > 0).sum()
    degraded = (df['clip_improvement'] < 0).sum()
    unchanged = (df['clip_improvement'] == 0).sum()
    
    categories = ['Improved', 'Degraded', 'Unchanged']
    counts = [improved, degraded, unchanged]
    colors = ['green', 'red', 'gray']
    
    axes[1].bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')
    axes[1].set_ylabel('Count')
    axes[1].set_title('Correction Effectiveness')
    axes[1].grid(True, alpha=0.3, axis='y')
    for i, (cat, count) in enumerate(zip(categories, counts)):
        axes[1].text(i, count + len(df) * 0.01, f'{count}\n({count/len(df)*100:.1f}%)', 
                     ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '2_clip_improvement_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: 2_clip_improvement_analysis.png")
    
    # 3. HCS Score Analysis (if available)
    if 'hcs_score' in df.columns and df['hcs_score'].notna().any():
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # HCS distribution
        hcs_scores = df['hcs_score'].dropna()
        axes[0, 0].hist(hcs_scores, bins=30, alpha=0.7, color='teal', edgecolor='black')
        axes[0, 0].axvline(x=hcs_scores.mean(), color='red', linestyle='--', linewidth=2, 
                          label=f'Mean: {hcs_scores.mean():.3f}')
        axes[0, 0].set_xlabel('HCS Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('HCS Score Distribution')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # HCS vs CLIP Improvement scatter
        valid_idx = df['hcs_score'].notna()
        axes[0, 1].scatter(df.loc[valid_idx, 'hcs_score'], 
                          df.loc[valid_idx, 'clip_improvement'], 
                          alpha=0.5, s=20)
        axes[0, 1].set_xlabel('HCS Score')
        axes[0, 1].set_ylabel('CLIP Improvement')
        axes[0, 1].set_title('HCS Score vs CLIP Improvement')
        axes[0, 1].grid(True, alpha=0.3)
        
        # HCS categories
        high = (hcs_scores >= 0.7).sum()
        medium = ((hcs_scores >= 0.4) & (hcs_scores < 0.7)).sum()
        low = (hcs_scores < 0.4).sum()
        
        categories = ['High (‚â•0.7)', 'Medium (0.4-0.7)', 'Low (<0.4)']
        counts = [high, medium, low]
        colors_hcs = ['green', 'orange', 'red']
        
        axes[1, 0].bar(categories, counts, color=colors_hcs, alpha=0.7, edgecolor='black')
        axes[1, 0].set_ylabel('Count')
        axes[1, 0].set_title('HCS Score Categories')
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        for i, (cat, count) in enumerate(zip(categories, counts)):
            axes[1, 0].text(i, count + len(hcs_scores) * 0.01, f'{count}\n({count/len(hcs_scores)*100:.1f}%)', 
                           ha='center', va='bottom', fontweight='bold')
        
        # HCS vs CLIP Generated scatter
        axes[1, 1].scatter(df.loc[valid_idx, 'hcs_score'], 
                          df.loc[valid_idx, 'clip_sim_generated'], 
                          alpha=0.5, s=20, label='Generated')
        axes[1, 1].scatter(df.loc[valid_idx, 'hcs_score'], 
                          df.loc[valid_idx, 'clip_sim_corrected'], 
                          alpha=0.5, s=20, label='Corrected')
        axes[1, 1].set_xlabel('HCS Score')
        axes[1, 1].set_ylabel('CLIP Similarity')
        axes[1, 1].set_title('HCS Score vs CLIP Similarity')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, '3_hcs_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì Saved: 3_hcs_analysis.png")
    
    # 4. Text Length Analysis
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Length distribution
    axes[0].hist(df['generated_length'], bins=30, alpha=0.6, label='Generated', color='blue')
    axes[0].hist(df['corrected_length'], bins=30, alpha=0.6, label='Corrected', color='green')
    axes[0].hist(df['ground_truth_length'], bins=30, alpha=0.6, label='Ground Truth', color='orange')
    axes[0].set_xlabel('Text Length (characters)')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Text Length Distribution')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Length comparison box plot
    length_data = [df['generated_length'], df['corrected_length'], df['ground_truth_length']]
    axes[1].boxplot(length_data, labels=['Generated', 'Corrected', 'Ground Truth'])
    axes[1].set_ylabel('Text Length (characters)')
    axes[1].set_title('Text Length Comparison')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '4_text_length_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: 4_text_length_analysis.png")
    
    # 5. Correlation Matrix
    numeric_cols = ['clip_sim_generated', 'clip_sim_corrected', 'clip_sim_ground', 'clip_improvement']
    if 'hcs_score' in df.columns and df['hcs_score'].notna().any():
        numeric_cols.append('hcs_score')
    
    if len(numeric_cols) > 1:
        corr_matrix = df[numeric_cols].corr()
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, 
                    square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
        ax.set_title('Correlation Matrix of Metrics')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, '5_correlation_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì Saved: 5_correlation_matrix.png")
    
    # 6. Top/Bottom Performers
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # Top 10 improvements
    top_improvements = df.nlargest(10, 'clip_improvement')[['image', 'clip_improvement', 'clip_sim_generated', 'clip_sim_corrected']]
    y_pos = np.arange(len(top_improvements))
    axes[0].barh(y_pos, top_improvements['clip_improvement'], color='green', alpha=0.7)
    axes[0].set_yticks(y_pos)
    axes[0].set_yticklabels([f"{row['image']}" for _, row in top_improvements.iterrows()], fontsize=8)
    axes[0].set_xlabel('CLIP Improvement')
    axes[0].set_title('Top 10 Samples by CLIP Improvement')
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # Bottom 10 (most degraded)
    bottom_improvements = df.nsmallest(10, 'clip_improvement')[['image', 'clip_improvement', 'clip_sim_generated', 'clip_sim_corrected']]
    y_pos = np.arange(len(bottom_improvements))
    axes[1].barh(y_pos, bottom_improvements['clip_improvement'], color='red', alpha=0.7)
    axes[1].set_yticks(y_pos)
    axes[1].set_yticklabels([f"{row['image']}" for _, row in bottom_improvements.iterrows()], fontsize=8)
    axes[1].set_xlabel('CLIP Improvement')
    axes[1].set_title('Bottom 10 Samples by CLIP Improvement')
    axes[1].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '6_top_bottom_performers.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: 6_top_bottom_performers.png")
    
    print(f"‚úÖ All visualizations saved to {output_dir}/")


def generate_report(stats: Dict, df: pd.DataFrame, output_path: str):
    """Generate a comprehensive text report."""
    print(f"üìù Generating evaluation report...")
    
    report = []
    report.append("=" * 80)
    report.append("WOODPECKER EVALUATION REPORT")
    report.append("=" * 80)
    report.append("")
    
    # Basic Statistics
    report.append("## BASIC STATISTICS")
    report.append("-" * 80)
    report.append(f"Total Samples: {stats['total_samples']}")
    report.append(f"Unique Images: {stats['unique_images']}")
    report.append("")
    
    # CLIP Similarity Statistics
    report.append("## CLIP SIMILARITY STATISTICS")
    report.append("-" * 80)
    report.append("\n### Generated Captions:")
    report.append(f"  Mean: {stats['clip_generated']['mean']:.4f}")
    report.append(f"  Std:  {stats['clip_generated']['std']:.4f}")
    report.append(f"  Min:  {stats['clip_generated']['min']:.4f}")
    report.append(f"  Max:  {stats['clip_generated']['max']:.4f}")
    report.append(f"  Median: {stats['clip_generated']['median']:.4f}")
    
    report.append("\n### Corrected Captions:")
    report.append(f"  Mean: {stats['clip_corrected']['mean']:.4f}")
    report.append(f"  Std:  {stats['clip_corrected']['std']:.4f}")
    report.append(f"  Min:  {stats['clip_corrected']['min']:.4f}")
    report.append(f"  Max:  {stats['clip_corrected']['max']:.4f}")
    report.append(f"  Median: {stats['clip_corrected']['median']:.4f}")
    
    report.append("\n### Ground Truth:")
    report.append(f"  Mean: {stats['clip_ground']['mean']:.4f}")
    report.append(f"  Std:  {stats['clip_ground']['std']:.4f}")
    report.append(f"  Min:  {stats['clip_ground']['min']:.4f}")
    report.append(f"  Max:  {stats['clip_ground']['max']:.4f}")
    report.append(f"  Median: {stats['clip_ground']['median']:.4f}")
    
    # Improvement Statistics
    report.append("\n## CORRECTION EFFECTIVENESS")
    report.append("-" * 80)
    report.append(f"Mean CLIP Improvement: {stats['clip_improvement']['mean']:.4f}")
    report.append(f"Std CLIP Improvement:  {stats['clip_improvement']['std']:.4f}")
    report.append(f"\nImproved Samples:     {stats['clip_improvement']['improved_count']} ({stats['clip_improvement']['improved_percentage']:.2f}%)")
    report.append(f"Degraded Samples:     {stats['clip_improvement']['degraded_count']} ({stats['clip_improvement']['degraded_percentage']:.2f}%)")
    report.append(f"Unchanged Samples:    {stats['clip_improvement']['unchanged_count']}")
    report.append(f"\n### Statistical Tests:")
    report.append(f"Paired t-test statistic: {stats['clip_improvement']['t_statistic']:.4f}")
    report.append(f"P-value: {stats['clip_improvement']['p_value']:.6f}")
    report.append(f"Significant (p < 0.05): {'Yes' if stats['clip_improvement']['significant'] else 'No'}")
    report.append(f"Effect Size (Cohen's d): {stats['clip_improvement']['cohens_d']:.4f}")
    
    # HCS Statistics
    if 'hcs' in stats:
        report.append("\n## HCS SCORE STATISTICS")
        report.append("-" * 80)
        report.append(f"Mean HCS Score: {stats['hcs']['mean']:.4f}")
        report.append(f"Std HCS Score:  {stats['hcs']['std']:.4f}")
        report.append(f"Min HCS Score:   {stats['hcs']['min']:.4f}")
        report.append(f"Max HCS Score:   {stats['hcs']['max']:.4f}")
        report.append(f"Median HCS Score: {stats['hcs']['median']:.4f}")
        report.append(f"Valid HCS Scores: {stats['hcs']['valid_count']}")
        report.append(f"Missing HCS Scores: {stats['hcs']['missing_count']}")
        
        report.append("\n### HCS Categories:")
        report.append(f"  High Confidence (‚â•0.7):   {stats['hcs_categories']['high_confidence']}")
        report.append(f"  Medium Confidence (0.4-0.7): {stats['hcs_categories']['medium_confidence']}")
        report.append(f"  Low Confidence (<0.4):     {stats['hcs_categories']['low_confidence']}")
    
    # Text Length Statistics
    report.append("\n## TEXT LENGTH STATISTICS")
    report.append("-" * 80)
    report.append(f"Generated Text - Mean: {stats['text_lengths']['generated']['mean']:.1f}, Median: {stats['text_lengths']['generated']['median']:.1f}")
    report.append(f"Corrected Text - Mean: {stats['text_lengths']['corrected']['mean']:.1f}, Median: {stats['text_lengths']['corrected']['median']:.1f}")
    report.append(f"Ground Truth - Mean: {stats['text_lengths']['ground_truth']['mean']:.1f}, Median: {stats['text_lengths']['ground_truth']['median']:.1f}")
    
    # Correction Statistics
    report.append("\n## CORRECTION STATISTICS")
    report.append("-" * 80)
    report.append(f"Samples with Text Changes: {stats['correction_stats']['changed_count']} ({stats['correction_stats']['changed_percentage']:.2f}%)")
    report.append(f"Samples Unchanged: {stats['correction_stats']['unchanged_count']}")
    
    # Top Performers
    report.append("\n## TOP 10 IMPROVEMENTS")
    report.append("-" * 80)
    top_10 = df.nlargest(10, 'clip_improvement')[['image', 'clip_improvement', 'clip_sim_generated', 'clip_sim_corrected']]
    for idx, row in top_10.iterrows():
        report.append(f"{row['image']}: {row['clip_improvement']:.4f} (Generated: {row['clip_sim_generated']:.4f} ‚Üí Corrected: {row['clip_sim_corrected']:.4f})")
    
    report.append("\n## BOTTOM 10 IMPROVEMENTS")
    report.append("-" * 80)
    bottom_10 = df.nsmallest(10, 'clip_improvement')[['image', 'clip_improvement', 'clip_sim_generated', 'clip_sim_corrected']]
    for idx, row in bottom_10.iterrows():
        report.append(f"{row['image']}: {row['clip_improvement']:.4f} (Generated: {row['clip_sim_generated']:.4f} ‚Üí Corrected: {row['clip_sim_corrected']:.4f})")
    
    report.append("\n" + "=" * 80)
    report.append("End of Report")
    report.append("=" * 80)
    
    # Write report
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"‚úÖ Report saved to {output_path}")


def save_summary_csv(df: pd.DataFrame, stats: Dict, output_path: str):
    """Save summary statistics to CSV."""
    print(f"üíæ Saving summary CSV...")
    
    # Create summary DataFrame
    summary_data = {
        'Metric': [
            'Total Samples',
            'Unique Images',
            'CLIP Generated Mean',
            'CLIP Corrected Mean',
            'CLIP Ground Truth Mean',
            'CLIP Improvement Mean',
            'Improved Count',
            'Improved Percentage',
            'Degraded Count',
            'Degraded Percentage',
            'Unchanged Count',
        ],
        'Value': [
            len(df),
            df['image'].nunique(),
            df['clip_sim_generated'].mean(),
            df['clip_sim_corrected'].mean(),
            df['clip_sim_ground'].mean(),
            df['clip_improvement'].mean(),
            (df['clip_improvement'] > 0).sum(),
            (df['clip_improvement'] > 0).sum() / len(df) * 100,
            (df['clip_improvement'] < 0).sum(),
            (df['clip_improvement'] < 0).sum() / len(df) * 100,
            (df['clip_improvement'] == 0).sum(),
        ]
    }
    
    if 'hcs_score' in df.columns and df['hcs_score'].notna().any():
        hcs_scores = df['hcs_score'].dropna()
        summary_data['Metric'].extend([
            'HCS Mean',
            'HCS Std',
            'HCS Min',
            'HCS Max',
            'HCS High Confidence Count',
            'HCS Medium Confidence Count',
            'HCS Low Confidence Count',
        ])
        summary_data['Value'].extend([
            hcs_scores.mean(),
            hcs_scores.std(),
            hcs_scores.min(),
            hcs_scores.max(),
            (hcs_scores >= 0.7).sum(),
            ((hcs_scores >= 0.4) & (hcs_scores < 0.7)).sum(),
            (hcs_scores < 0.4).sum(),
        ])
    
    # Add statistical test results
    if 'clip_improvement' in stats:
        summary_data['Metric'].extend([
            'T-Statistic',
            'P-Value',
            'Cohen\'s d',
            'Statistically Significant'
        ])
        summary_data['Value'].extend([
            stats['clip_improvement']['t_statistic'],
            stats['clip_improvement']['p_value'],
            stats['clip_improvement']['cohens_d'],
            'Yes' if stats['clip_improvement']['significant'] else 'No'
        ])
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(output_path, index=False)
    print(f"‚úÖ Summary CSV saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Evaluate Woodpecker results and generate visualizations")
    parser.add_argument("--input-jsonl", required=True, help="Input JSONL file with results")
    parser.add_argument("--output-dir", default="results/evaluation", help="Output directory for visualizations and reports")
    parser.add_argument("--report-name", default="evaluation_report.txt", help="Name of the text report file")
    parser.add_argument("--csv-name", default="summary_statistics.csv", help="Name of the summary CSV file")
    args = parser.parse_args()
    
    # Load results
    df = load_results(args.input_jsonl)
    
    # Compute statistics
    print("üìä Computing statistics...")
    stats, df = compute_statistics(df)
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Generate visualizations
    create_visualizations(df, stats, args.output_dir)
    
    # Generate report
    report_path = os.path.join(args.output_dir, args.report_name)
    generate_report(stats, df, report_path)
    
    # Save summary CSV
    csv_path = os.path.join(args.output_dir, args.csv_name)
    save_summary_csv(df, stats, csv_path)
    
    # Print summary
    print("\n" + "=" * 80)
    print("EVALUATION SUMMARY")
    print("=" * 80)
    print(f"Total Samples: {stats['total_samples']}")
    print(f"CLIP Improvement Mean: {stats['clip_improvement']['mean']:.4f}")
    print(f"Improved: {stats['clip_improvement']['improved_count']} ({stats['clip_improvement']['improved_percentage']:.2f}%)")
    print(f"Degraded: {stats['clip_improvement']['degraded_count']} ({stats['clip_improvement']['degraded_percentage']:.2f}%)")
    if 'hcs' in stats:
        print(f"HCS Mean: {stats['hcs']['mean']:.4f}")
    print(f"\n‚úÖ All outputs saved to: {args.output_dir}/")
    print("=" * 80)


if __name__ == "__main__":
    main()

